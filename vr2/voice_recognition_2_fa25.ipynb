{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307538ae-ba40-4d77-b623-e53c1b2b6933",
   "metadata": {},
   "source": [
    "# Voice Recognition Lab 2: Extending SVD/PCA Classification with Spectral Analysis\n",
    "\n",
    "### EECS 16A: Foundations of Signals, Dynamical Systems, and Information Processing, Fall 2025\n",
    "\n",
    "Junha Kim, Jessica Fan, Savit Bhat, Jack Kang (Fall 2024).\n",
    "\n",
    "Sonia Chacon (Spring 2025)\n",
    "\n",
    "Tanya Deniz Ipek (Fall 2025)\n",
    "\n",
    "This lab was heavily inspired by previous EECS16B lab 8, written by Nathaniel Mailoa, Emily Naviasky, et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b0202-8df8-41c7-a245-81dd23600647",
   "metadata": {
    "tags": []
   },
   "source": [
    "* [Task 1: Data Preprocessing](#task1)\n",
    "* [Task 2: Spectral Analysis](#task2)\n",
    "* [Task 3: Data Reshaping](#task3)\n",
    "* [Task 4: PCA via SVD](#task4)\n",
    "* [Task 5: Testing your Classifier](#task5)\n",
    "* [Task 6: Live Classify](#task6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fce195-2dd6-464c-90d1-e9f68a625eb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Before you start, please migrate your recording CSV file from last week. Also, have the notebook from last week handy, as you might be copying code from it to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef9ce3-101b-4679-b5a1-f566bca746b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sounddevice\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm.notebook import trange\n",
    "from IPython import display\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.signal import stft, spectrogram\n",
    "%matplotlib inline\n",
    "\n",
    "cm = ['blue', 'red', 'green', 'orange', 'black', 'purple']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea7803-5f16-4e7c-9678-b2ffef993464",
   "metadata": {},
   "source": [
    "<a id='task1'></a>\n",
    "# <span style=\"color:navy\"> Task 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0869a7-62a4-4c8e-bc41-2b599a6531d3",
   "metadata": {},
   "source": [
    "We will repeat the data preprocessing step from lab 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33015c0b-b259-46cf-81d7-12e6a65c8c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: If you recorded an additional word for VR1, replace the empty string with that word.\n",
    "# If not, please remove the empty string.\n",
    "all_words_arr = ['jack', 'jason', 'jessica', 'principalcomponent', '...']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddcdcf-db71-4132-9fea-fbed6cbc6ff8",
   "metadata": {},
   "source": [
    "Let's begin by splitting our data into a training and testing set with a 70/30 split. Run the code below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87887e75-56b7-438a-afae-cab2bdf19fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "train_test_split_ratio = 0.7\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "# Build the dictionary of train and test samples.\n",
    "for i in range(len(all_words_arr)):\n",
    "    word_raw = utils.read_csv(\"{}.csv\".format(all_words_arr[i]))\n",
    "    word_raw_train, word_raw_test = utils.train_test_split(word_raw, train_test_split_ratio)\n",
    "    train_dict[all_words_arr[i]] = word_raw_train\n",
    "    test_dict[all_words_arr[i]] = word_raw_test\n",
    "\n",
    "# Count the minimum number of samples you took across the six recorded words. These variables might be useful for you!\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], test_dict.values())))\n",
    "\n",
    "# Crop the number of samples for each word to the minimum number so all words have the same number of samples.\n",
    "for key, raw_word in train_dict.items():\n",
    "    train_dict[key] = raw_word[:num_samples_train,:]\n",
    "\n",
    "for key, raw_word in test_dict.items():\n",
    "    test_dict[key] = raw_word[:num_samples_test,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5b5db-b423-48e0-98b7-be2512ed8b49",
   "metadata": {},
   "source": [
    "Align the recordings as we did last week. Paste in the `align_recording` function you wrote last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd516e9-aea6-4329-8446-27900e584496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_recording(recording, length, pre_length, threshold, envelope=False):\n",
    "    \"\"\"\n",
    "    align a single audio samples according to the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        recording (np.ndarray): a single audio sample.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        envelope (bool): if True, use enveloping.\n",
    "    \n",
    "    Returns:\n",
    "        aligned recording.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: PASTE IN YOUR ALIGN_RECORDING FUNCTION FROM LAST WEEK\n",
    "    \n",
    "    if envelope:\n",
    "        recording = utils.envelope(recording, 5400, 100)\n",
    "    \n",
    "    # Find the threshold\n",
    "    recording_threshold = threshold * np.max(recording)\n",
    "\n",
    "    # TODO: Use recording_threshold, length, and prelength to cut the snippet to the correct length\n",
    "    # we note where the recording magnitude first exceeds recording_threshold.\n",
    "    # then, we leave prelength number of samples before the crossing of the threshold, which is where our recording starts.\n",
    "    # we cut the recording so that it's 'length' samples away from the start of the recording.\n",
    "    \n",
    "    i = 0\n",
    "    while ... : # YOUR CODE HERE\n",
    "        i += 1\n",
    "\n",
    "    snippet_start = np.clip(i - pre_length, a_min=0, a_max=len(recording) - length)\n",
    "    snippet = # YOUR CODE HERE\n",
    "\n",
    "    # TODO: Normalize the recording.\n",
    "    # \"Normalize\" in our case is dividing the signal by the maximum absolute value (different from taking the norm of a vector)\n",
    "    snippet_normalized = # YOUR CODE HERE\n",
    "    \n",
    "    return snippet_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5aa8e-3ae4-4e43-830f-0a0aed5ec0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_data(data, length, pre_length, threshold, envelope=False):\n",
    "    \"\"\"\n",
    "    align all audio samples in dataset. (apply align_recording to all rows of the data matrix)\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "    \n",
    "    Returns:\n",
    "        Matrix of aligned recordings.\n",
    "    \"\"\"\n",
    "    assert isinstance(data, np.ndarray) and len(data.shape) == 2, \"'data' must be a 2D matrix\"\n",
    "    assert isinstance(length, int) and length > 0, \"'length' of snippet must be an integer greater than 0\"\n",
    "    assert 0 <= threshold <= 1, \"'threshold' must be between 0 and 1\"\n",
    "    snippets = []\n",
    "\n",
    "    # Iterate over the rows in data\n",
    "    for recording in data:\n",
    "        snippets.append(align_recording(recording, length, pre_length, threshold, envelope))\n",
    "\n",
    "    return np.vstack(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82f958-ebf5-4286-a5e2-c9c0ee8b0c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(selected_words_arr, dict_raw, length, pre_length, threshold, plot=True, envelope=False):\n",
    "    \"\"\"\n",
    "    Process the raw data given parameters and return it. (wrapper function for align_data)\n",
    "    \n",
    "    Args:\n",
    "        dict_raw (np.ndarray): Raw data collected.\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        plot (boolean): Plot the dataset if true.\n",
    "            \n",
    "    Returns:\n",
    "        Processed data dictionary.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    word_number = 0\n",
    "    for key, word_raw in dict_raw.items():\n",
    "        word_processed = align_data(word_raw, length, pre_length, threshold, envelope=envelope)\n",
    "        processed_dict[key] = word_processed\n",
    "        if plot:\n",
    "            plt.plot(word_processed.T)\n",
    "            plt.title('Samples for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "            word_number += 1\n",
    "            plt.show()\n",
    "            \n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd156300-14c1-431d-9506-375346348106",
   "metadata": {},
   "source": [
    "Align your recordings. **NOTE: we want to set `envelope=False` for spectral analysis!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a9d82-fd08-4efd-80bf-2142d0b82a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Edit the parameters to get the best alignment.\n",
    "length = # YOUR CODE HERE\n",
    "pre_length = 400 # Modify this as necessary\n",
    "threshold = # YOUR CODE HERE\n",
    "\n",
    "# align training and test data\n",
    "processed_train_dict = process_data(all_words_arr, train_dict, length, pre_length, threshold, envelope=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584bcc1-72c1-4e1d-ad4b-745f55eed56f",
   "metadata": {},
   "source": [
    "<a id='task2'></a>\n",
    "# <span style=\"color:navy\">Task 2: Spectral Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c34a91-6397-46a7-ab45-fd55ca1dfecf",
   "metadata": {},
   "source": [
    "You have seen spectrograms previously in the Shazam lab. As a reminder, it's a DFT calculated on many small snippets of the signal in question. The spectrogram is a 2d plot that gives insights on both temporal and frequency information in the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a4fd1-171b-4d9d-91cf-6e605ac732e5",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Utilizing Spectrograms to extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10711d05-bd38-41e6-9d20-fc75e18028f5",
   "metadata": {},
   "source": [
    "By obtaining the spectrogram, we are able to capture the unique time-varying frequency footprint of the signal. After collecting these coefficients, we will be able to use PCA, a method of low-rank approximation, to convert the spectrogram coefficients into a basis that maximizes the amount of variance.\n",
    "\n",
    "Calculating the spectrogram first will allow PCA to identify variations in both frequency and temporal details. \n",
    "\n",
    "Generate a spectrogram for each recording below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980374fb-58ca-4805-ade8-03c144910623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the spectrogram of one recording.\n",
    "# hint: scipy.signal.spectrogram may be useful here. you can just invoke spectrogram() without writing scipy.signal\n",
    "def spectrogram_single_recording(data, sample_rate=5400, return_f_t=False):\n",
    "    \"\"\"\n",
    "    calculate spectrogram of one recording.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): single recording (row)\n",
    "        sample_rate (int): sampling rate in Hz\n",
    "        return_f_t (bool): indicate whether to only return Zxx or f,t,Zxx (for plotting)\n",
    "    Returns:\n",
    "        f (np.array): frequency index array\n",
    "        t (np.array): time index array\n",
    "        Zxx (np.array): array of 2D arrays (spectrogram result of each row)\n",
    "    \"\"\"\n",
    "    f, t, Zxx = # YOUR CODE HERE\n",
    "    \n",
    "    if return_f_t:\n",
    "        return f,t,Zxx\n",
    "    else:\n",
    "        return Zxx\n",
    "\n",
    "def spectrogram_recordings(values, sample_rate=5400, return_f_t=False):\n",
    "    \"\"\"\n",
    "    calculate spectrogram of multiple recordings.\n",
    "    \n",
    "    Args:\n",
    "        values (np.array): recordings matrix\n",
    "        sample_rate (int): sampling rate in Hz\n",
    "        return_f_t (bool): indicate whether to only return Zxx or f,t,Zxx (for plotting)\n",
    "    Returns:\n",
    "        f (np.array): frequency index array\n",
    "        t (np.array): time index array\n",
    "        Zxx (np.array): array of 2D arrays (spectrogram result of each row)\n",
    "    \"\"\"\n",
    "    spect_vals = []\n",
    "    for (word, recordings) in values:\n",
    "        for i in range(recordings.shape[0]):\n",
    "            f, t, Zxx = spectrogram_single_recording(recordings[i, :], sample_rate=sample_rate, return_f_t=True)\n",
    "            spect_vals.append(...) # YOUR CODE HERE\n",
    "\n",
    "    # Convert the list to a numpy array\n",
    "    if return_f_t:\n",
    "        return f,t,np.array(spect_vals)\n",
    "    else:\n",
    "        return np.array(spect_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cf5ad-dcc8-4a22-af27-429b1b77c2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, t, spectrogram_results = spectrogram_recordings(processed_train_dict.items(), return_f_t=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc09544-5172-4e8a-9ff8-10016143fc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the first spectrogram result\n",
    "plt.pcolormesh(t, f, np.abs(spectrogram_results[0]), shading='gouraud')\n",
    "plt.title('Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d03a74-ae6f-48b0-94c6-4a3545a2cbb6",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Mel-scaled Spectrograms\n",
    "\n",
    "The Mel-scaling is a frequency scaling that takes advantage of knowledge of the human auditory system. Rather than representing frequencies linearly, it uses this prior knowledge to compress higher frequencies and expand lower frequencies, similar to our ears. In other words, it reforms the frequency scaling to be more similar to the way humans perceive pitch. This has the potential to extract features that are more relevant to human auditory perception.\n",
    "\n",
    "In DFT (Discrete Fourier Transform), the frequency bins are dictated by the DFT frequency bins. In the Mel-scaled equivalent, the frequency bins use a new rescaled unit called 'Mels', which is what the `n_mels` argument for the function below denotes. Thus, the returned spectrogram result would be a 2D array of shape (# timesteps x # Mels) that contains the spectrogram value in each location.\n",
    "\n",
    "We will provide the functions needed for generating Mel-scaled spectrograms, so you can treat it as a black box if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117b3f7-db36-42e1-9ab0-bbd2698168b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run mel-scaled spectrogram on single recording\n",
    "def mel_spectrogram_single_recording(data, sample_rate=5400, n_fft=256, n_mels=100, return_f_t=False):\n",
    "    mel_filter = utils.mel_filter_bank(sample_rate, n_fft, n_mels)\n",
    "    f, t, spectrogram_result = spectrogram_single_recording(data, return_f_t=True)\n",
    "    mel_spectrogram_result = np.array(utils.apply_mel_filter([spectrogram_result], mel_filter))[0]\n",
    "    mel_freqs = utils.mel_frequencies(n_mels, sample_rate, n_fft)\n",
    "    if return_f_t:\n",
    "        return mel_freqs, t, mel_spectrogram_result\n",
    "    else:\n",
    "        return mel_spectrogram_result\n",
    "\n",
    "# run mel-scaled spectrogram on entire dataset\n",
    "def mel_spectrogram_recordings(vals, sample_rate=5400, n_fft=256, n_mels=100, return_f_t=False):\n",
    "    mel_filter = utils.mel_filter_bank(sample_rate, n_fft, n_mels)\n",
    "    f, t, spectrogram_results = spectrogram_recordings(vals, return_f_t=True)\n",
    "    mel_spectrogram_results = np.array(utils.apply_mel_filter(spectrogram_results, mel_filter))\n",
    "    mel_freqs = utils.mel_frequencies(n_mels, sample_rate, n_fft)\n",
    "    if return_f_t:\n",
    "        return mel_freqs, t, mel_spectrogram_results\n",
    "    else:\n",
    "        return mel_spectrogram_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd489-0525-4532-8bd6-e463f2312003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mel_freqs, t, mel_spectrogram_results = mel_spectrogram_recordings(processed_train_dict.items(), return_f_t=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18964c7a-2c75-42a2-8f0c-4954c5eaece8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.pcolormesh(t, mel_freqs, mel_spectrogram_results[0], shading='gouraud')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.ylabel('Frequency [Mels]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e39499-9696-4a49-a10e-88c9615999a6",
   "metadata": {},
   "source": [
    "<a id='task3'></a>\n",
    "# <span style=\"color:navy\"> Task 3: Data Reshaping</span>\n",
    "\n",
    "\n",
    "Currently, our spectrogram matrices (both regular and Mel-scaled) are 2d (time and frequency). This doesn't lend itself well to PCA, which requires a single vector per measurement. \n",
    "\n",
    "Therefore, we must flatten our array to a 1d format. There are many ways to accomplish this--we will be exploring and attempting some of the options below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58fe7a-f98d-4192-be05-b0d727eb7654",
   "metadata": {},
   "source": [
    "**Sanity check**: What do each axis in our spectrogram_results array represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b0bc6-fee5-40df-a0d8-031adc901b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectrogram_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bc852-7364-40fa-8c0c-945a5058e4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mel_spectrogram_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9cb8aa-48f8-470b-b60e-e4425a6a2c84",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Flattening\n",
    "\n",
    "In flattening, we are simply taking all the elements from the matrix and place them into a single sequence, thereby having no data loss. Keeping in mind that the two dimensions are time and frequency, we might choose to flatten row-wise, or column-wise. We may even decide to use a different approach, zig-zagging through the matrix to flatten it. For now, we will use row-wise flattening, but we encourage you to experiment with approaches in the later part of the lab.\n",
    "\n",
    "<img src=\"images/flatten.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c89de-bb7d-4575-9eb9-07260b93ab75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flatten the matrix row-wise.\n",
    "def apply_flattening_single_recording(data):\n",
    "    return # YOUR CODE HERE hint: np.reshape may be helpful here.\n",
    "\n",
    "# flatten a list of matrices row-wise.\n",
    "def apply_flattening(vals):\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf38fa8",
   "metadata": {},
   "source": [
    "Test these functions by running the cells below. Does the shape match what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test apply_flattening_single_recording\n",
    "x = np.array([[1,1], [2,2], [3,3]])\n",
    "y = apply_flattening_single_recording(x)\n",
    "print(\"Input:\", x)\n",
    "print(\"Result:\", y)\n",
    "print(\"Shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test apply_flattening\n",
    "a = np.array([x, 2*x, 3*x]) # create a list of matrices\n",
    "b = apply_flattening(a)\n",
    "print(\"Input:\", a)\n",
    "print(\"Result:\", b)\n",
    "print(\"Shape:\", b.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90707a-6e35-4cbd-b970-ec69e6b56036",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Aggregation of standard deviation / variance features over frames\n",
    "\n",
    "Another method to convert our matrix to a vector is to use mean and variances over frequency bins for each time frame. This does lose some information, but very specific frequency information is likely redundant for a simple voice classification scheme. Generally, the memory tradeoff for this feature is worth it.\n",
    "\n",
    "<img src=\"images/aggregate.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f673830-1ce8-4ebe-9fbf-d0e8723faf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Compute mean and variance over frequency bins for each time frame\n",
    "\n",
    "# apply aggregation for one recording\n",
    "def apply_aggregation_single_recording(data):\n",
    "    \n",
    "    # hint 1: use np.mean and np.var\n",
    "    # hint 2: the time varies down the rows of data, so what axis should you take the mean/var from?\n",
    "    mean_feature, variance_feature = # YOUR CODE HERE\n",
    "\n",
    "    # Stack features to get a combined feature set\n",
    "    return # YOUR CODE HERE hint: use np.concatenate\n",
    "\n",
    "# apply aggregation for multiple recordings\n",
    "def apply_aggregation(vals):\n",
    "    \n",
    "    # hint: you should use the same functions as above, but how should the axis argument change since\n",
    "    #       vals is a list of recordings?\n",
    "    mean_features, variance_features = # YOUR CODE HERE\n",
    "\n",
    "    # Stack features to get a combined feature set\n",
    "    return # YOUR CODE HERE (same as above, but you might have to specify a different axis argument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dc614-0130-472b-b43c-5df070f50142",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Computing all 4 possible configurations for spectral analysis + reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e449f-5da8-46a7-a181-d6fa6f7b80ce",
   "metadata": {},
   "source": [
    "We have two methods of spectrogram generation (Mel-scaled spectrograms and regular spectrograms) and two methods of data aggregation (regular flattening vs mean & variance aggregation). That gives us 4 possible combinations, so let's generate a processed dataset for all 4 cases so we can compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac9bb5-3c8f-486b-a49e-9d72f1a84062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the functions we wrote above!\n",
    "# Remember the data we are using is from either spectrogram_results or mel_spectrogram_results!\n",
    "\n",
    "# TODO: mel scaled spectrogram + flattening\n",
    "processed_A_mel_flattening = # YOUR CODE HERE\n",
    "print(f\"processed_A_mel_flattening shape: {processed_A_mel_flattening.shape}\")\n",
    "\n",
    "# TODO: regular spectrogram + flattening\n",
    "processed_A_spectrogram_flattening = # YOUR CODE HERE\n",
    "print(f\"processed_A_spectrogram_flattening shape: {processed_A_spectrogram_flattening.shape}\")\n",
    "\n",
    "# TODO: mel scaled spectrogram + aggregation\n",
    "processed_A_mel_aggregated = # YOUR CODE HERE\n",
    "print(f\"processed_A_mel_aggregated shape: {processed_A_mel_aggregated.shape}\")\n",
    "\n",
    "# TODO: regular spectrogram + aggregation\n",
    "processed_A_spectrogram_aggregated = # YOUR CODE HERE\n",
    "print(f\"processed_A_spectrogram_aggregated shape: {processed_A_spectrogram_aggregated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dd5fb-a2d3-41d0-bfd3-dc83bac89bec",
   "metadata": {},
   "source": [
    "<a id='task4'></a>\n",
    "# <span style=\"color:navy\">Task 4: PCA via SVD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fab25e-7502-4b27-b605-d46207aa52d1",
   "metadata": {},
   "source": [
    "Now we will be repeating our PCA steps from last week's lab. Refer to the code you've written to complete this section. In the empty `processed_A`, choose one of the four processed datasets from above. Rerun this section for different cases to compare the clustering characteristics and results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c246f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one of the four processed_A we generated above\n",
    "processed_A = # YOUR CODE HERE\n",
    "# demean the matrix A\n",
    "mean_vec = # YOUR CODE HERE\n",
    "demeaned_A = # YOUR CODE HERE\n",
    "\n",
    "# Take the SVD of matrix demeaned_A\n",
    "U, S, Vt = # YOUR CODE HERE\n",
    "\n",
    "# Plot out the sigma values\n",
    "plt.figure()\n",
    "plt.stem(S)\n",
    "plt.title(\"Stem Plot of Sigma Values\")\n",
    "\n",
    "# Plot the principal component(s)\n",
    "new_basis = # YOUR CODE HERE\n",
    "plt.figure()\n",
    "plt.plot(new_basis)\n",
    "plt.title(\"New Basis Vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dbbb1-5b59-4152-8de2-ac5478d25c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project the data onto the new basis\n",
    "proj = # YOUR CODE HERE hint: np.dot() may help\n",
    "\n",
    "# Determine the centroids of each cluster\n",
    "centroids = []\n",
    "for i in range(len(all_words_arr)):\n",
    "    centroid = np.mean(proj[i*num_samples_train:(i + 1)* num_samples_train], axis=0)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "\n",
    "######################\n",
    "# Plot the centroids #\n",
    "######################\n",
    "centroid_list = np.vstack(centroids)\n",
    "colors = cm[:(len(centroids))]\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "plt.show()\n",
    "for i, centroid in enumerate(centroid_list):\n",
    "    print('Centroid {} is at: {}'.format(i, str(centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017201f-9665-472d-9790-5440153add83",
   "metadata": {},
   "source": [
    "<a id='task5'></a>\n",
    "# <span style=\"color:navy\"> Task 5: Testing your Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ae18f-d009-49c4-93ed-bb1992fe9b5b",
   "metadata": {},
   "source": [
    "Great! Now that we have the means (centroid) for each word, let's evaluate performance. Recall that we will classify each data point according to the centroid with the least Euclidian distance to it.\n",
    "\n",
    "Before we perform classification, we need to do the same preprocessing to the test data that we did to the training data (enveloping, demeaning, projecting onto the PCA basis). You have already written most of the code for this part. However, note the difference in variable names as we are now working with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0eee3-efcb-4279-adb8-96f457266542",
   "metadata": {},
   "source": [
    "First let's look at what our raw test data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7028352-d362-4c53-998f-205124eaa7b3",
   "metadata": {},
   "source": [
    "## <span style=\"color:navy\"> Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ffcaf-1b33-4fbf-beb9-25589e511ef9",
   "metadata": {},
   "source": [
    "Perform enveloping and trimming of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0cb4f-cfda-46c5-bc32-7cec6ff90c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_test_dict = process_data(all_words_arr, test_dict, length, pre_length, threshold, envelope=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa005c-6616-46c6-83ac-7bd6b7086b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run spectrogram and mel spectrogram on the test set as well\n",
    "spectrogram_results_test = spectrogram_recordings(processed_test_dict.items(), return_f_t=False)\n",
    "mel_spectrogram_results_test = mel_spectrogram_recordings(processed_test_dict.items(), return_f_t=False)\n",
    "\n",
    "# generate the four possible configurations for the test data as well\n",
    "\n",
    "# TODO: mel scaled spectrogram + flattening\n",
    "processed_A_mel_flattening_test = # YOUR CODE HERE\n",
    "print(f\"processed_A_mel_flattening_test shape: {processed_A_mel_flattening_test.shape}\")\n",
    "\n",
    "# TODO: regular spectrogram + flattening\n",
    "processed_A_spectrogram_flattening_test = # YOUR CODE HERE\n",
    "print(f\"processed_A_spectrogram_flattening_test shape: {processed_A_spectrogram_flattening_test.shape}\")\n",
    "\n",
    "# TODO: mel scaled spectrogram + aggregation\n",
    "processed_A_mel_aggregated_test = # YOUR CODE HERE\n",
    "print(f\"processed_A_mel_aggregated_test shape: {processed_A_mel_aggregated_test.shape}\")\n",
    "\n",
    "# TODO: regular spectrogram + aggregation\n",
    "processed_A_spectrogram_aggregated_test = # YOUR CODE HERE\n",
    "print(f\"processed_A_spectrogram_aggregated_test shape: {processed_A_spectrogram_aggregated_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd8638-c9ef-4061-a743-f6dbba0f0104",
   "metadata": {},
   "source": [
    "Now we will project our processed test dataset the same way we did as before. As a reminder, we precomputed the mean vector $ \\bar{x}_{\\text{proj}} $ to save storage in our test classification and live classification:\n",
    "\n",
    "$$ (x - \\bar{x})P = xP - \\bar{x}P = xP - \\bar{x}_{\\text{proj}} $$ \n",
    "$$ \\bar{x}_{\\text{proj}} = \\bar{x}P $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92157b53-221c-43f5-95fc-13a096e59c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose the processed_A_test combination from above that matches the same combination as the training data\n",
    "processed_A_test = # YOUR CODE HERE\n",
    "\n",
    "# precompute the projected mean vector\n",
    "projected_mean_vec = # YOUR CODE HERE\n",
    "\n",
    "# Project the data onto the new basis and demean it with the precomputed projected mean vector\n",
    "proj = # YOUR CODE HERE hint: np.dot() may help.\n",
    "\n",
    "# Determine the centroids of each cluster\n",
    "centroids = []\n",
    "for i in range(len(all_words_arr)):\n",
    "    centroid = np.mean(proj[i*num_samples_test:(i + 1)* num_samples_test], axis=0)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "######################\n",
    "# Plot the centroids #\n",
    "######################\n",
    "centroid_list = np.vstack(centroids)\n",
    "colors = cm[:(len(centroids))]\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_test:num_samples_test*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_test:num_samples_test*(i+1),1], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "plt.show()\n",
    "for i, centroid in enumerate(centroid_list):\n",
    "    print('Centroid {} is at: {}'.format(i, str(centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c57bca-b4c2-4c50-a1fa-22425cae3e45",
   "metadata": {},
   "source": [
    "Implement the classify function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34cad2-bfea-46e1-beeb-a37f59d6dc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify(data_point, new_basis, projected_mean_vec, centroids):\n",
    "    \"\"\"Classifies a new voice recording into a word.\n",
    "    \n",
    "    Args:\n",
    "        data_point: new data point vector before demeaning and projection\n",
    "        new_basis: the new processed basis to project on\n",
    "        projected_mean_vec: the same projected_mean_vec as before\n",
    "    Returns:\n",
    "        Word number (should be in {1, 2, 3, 4} -> you might need to offset your indexing!)\n",
    "    Hint:\n",
    "        Remember to use 'projected_mean_vec'!\n",
    "        np.argmin(), and np.linalg.norm() may also help!\n",
    "    \"\"\"\n",
    "    # TODO: classify the demeaned data point by comparing its distance to the centroids\n",
    "    projected_data_point = # YOUR CODE HERE\n",
    "    demeaned = # YOUR CODE HERE\n",
    "    return all_words_arr[...] # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa97fec-87ac-422a-a8db-8f3d25d62d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try out the classification function\n",
    "print(classify(processed_A_test[0,:], new_basis, projected_mean_vec, centroids)) # Modify the row index of processed_A_test to use other vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e8a20-a2b4-4e3d-8679-5e2173b8cc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to classify the whole A matrix\n",
    "correct_counts = np.zeros(len(all_words_arr))\n",
    "\n",
    "for (row_num, data) in enumerate(processed_A_test):\n",
    "    word_num = row_num // num_samples_test\n",
    "    if classify(data, new_basis, projected_mean_vec, centroids) == all_words_arr[word_num]:\n",
    "        correct_counts[word_num] += 1\n",
    "        \n",
    "for i in range(len(correct_counts)):\n",
    "    print(\"Percent correct of word {} = {}%\".format(all_words_arr[i], 100 * correct_counts[i] / num_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700aa31-5528-4691-a3b6-8252ec8bc49f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='task6'></a>\n",
    "# <span style=\"color:navy\">Task 6: Testing the classifier: Real Time</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c6caa-26a1-4250-b65a-dbe57597d4bd",
   "metadata": {},
   "source": [
    "**Once you finish Task 5 with satisfactory accuracies, please come to the Cory 140 lab to try out your classifier!**\n",
    "\n",
    "Now, we'll be testing the classifier in real time. Run the script below, and when prompted, say one of your chosen words. \n",
    "\n",
    "NOTE: Do not worry if your detector does not work as intended for some words. If you decided to record your own word in Task 1, it is likely your classifier will only detect your own word. This is likely due to the fact that your recording will have different frequencies compared to the recordings we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c12d1c-346f-42ea-b861-6fcaeba9dccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rate = 5400  # Sample rate\n",
    "chunk = 1024  # Chunk size\n",
    "record_seconds = 3  # Record duration in seconds\n",
    "num_recordings = 50  # Total number of recordings needed\n",
    "recording_count = 0\n",
    "word = \"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Press Enter to start recording, or type 'stop' and then Enter to stop recording: \")\n",
    "\n",
    "    if user_input == '':\n",
    "        # Record audio\n",
    "        audio_recording = utils.record_audio(seconds=record_seconds, rate=rate, chunk=chunk)\n",
    "\n",
    "        # TODO: Preprocess the single data\n",
    "        aligned_audio_recording = # YOUR CODE HERE hint: use align_recording\n",
    "        \n",
    "        # TODO: \n",
    "        # run your chosen combo of spectrogram vs mel-spectrogram + flattening vs aggregation\n",
    "        # hint: you might want to use the single_recording version of the functions we wrote above\n",
    "        result = # YOUR CODE HERE\n",
    "        processed_audio_recording = # YOUR CODE HERE\n",
    "\n",
    "        # Run Classify\n",
    "        print(\"Classified Word: \" + classify(processed_audio_recording, new_basis, projected_mean_vec, centroids))\n",
    "        time.sleep(2)\n",
    "       \n",
    "    if user_input == 'stop':\n",
    "        display.clear_output()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fff5a",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">CHECKOFF</span>\n",
    "\n",
    "### When you are ready to get checked off, fill out the **[Checkoff Google Form](https://docs.google.com/forms/d/e/1FAIpQLScwrFoYRPPZ7eAhCnDLsIB7FFP2na2CgkW0RTa3A0Ii1Xf5NA/viewform)**\n",
    "\n",
    "- **Have all questions, code, and plots completed in this notebook.** Your TA will check all your PCA code and plots.\n",
    "- **Show your GSI that you've achieved 80% accuracy on your test data for all 4 words.**\n",
    "- Make sure to test **at least two** of the four possible combinations and be able to talk about the differences in accuracy.\n",
    "- **Show your GSI that you are able to classify live (Please show up to section for this).**\n",
    "- **Be prepared to answer conceptual questions about the lab.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
